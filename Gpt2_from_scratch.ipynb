{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tushar1344/Getting-and-Cleaning-Data-_Coursera-DS/blob/master/Gpt2_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgqVicPCbA9t",
        "outputId": "63748ab3-9645-4562-ff09-30d31b42cacb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read in GPT2 using Huggingface"
      ],
      "metadata": {
        "id": "WLotgtfBaBBv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IJyvAFHofKfH"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "# sd_hf = model_hf.state_dict()\n",
        "\n",
        "# for k,v in sd_hf.items():\n",
        "#   print(k,v.shape)"
      ],
      "metadata": {
        "id": "K2Gt3ZS1iAwT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "# plt.imshow(sd_hf[\"transformer.wpe.weight\"])"
      ],
      "metadata": {
        "id": "X7xfRH0zjMLX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 150])\n",
        "#plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 200])\n",
        "#plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 250])"
      ],
      "metadata": {
        "id": "rDTz2b_MlyQL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(sd_hf[\"transformer.h.1.attn.c_attn.weight\"][:300, :300],cmap = \"gray\")"
      ],
      "metadata": {
        "id": "Y3xvYVOIhNTi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline, set_seed\n",
        "\n",
        "# generator = pipeline('text-generation', model='gpt2')\n",
        "# set_seed(42)\n",
        "# generator(\"Hello, I'm a language model\", max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "id": "H-YVC-amki6D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building your own GPT2"
      ],
      "metadata": {
        "id": "bpOwuxSEaLom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass  #for building the config object\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "pOgbr0-VlqGw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "  block_size: int = 1024 #Context length\n",
        "  vocab_size: int = 50257 #Number of tokens\n",
        "  n_layer: int = 12 #Number of layers\n",
        "  n_head: int = 12  #Number of heads\n",
        "  n_embd: int = 768 #Embedding size\n"
      ],
      "metadata": {
        "id": "ylzzsptYmcSR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "  '''\n",
        "  A vanilla multi-head masked self-attention layer with a projection at the end\n",
        "  '''\n",
        "  def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # Key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # Regularization not present\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        # Not really a \"bias\", more of a mask, but following the OpenAI/HF naming though\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                             .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "        B, T, C = x.size()  # Batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        # Causal Self-Attention\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v  # (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # Re-assemble all head outputs side by side\n",
        "        y = self.c_proj(y)\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "GUGDXuTz36Eu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csa_obj = CausalSelfAttention(GPTConfig())"
      ],
      "metadata": {
        "id": "J6_ZDJywyex9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  '''MLP layer with GELU activation'''\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "    self.gelu = nn.GELU(approximate= 'tanh')\n",
        "    self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.c_fc(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.c_proj(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "y-sQmgcE2my2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_obj = MLP(GPTConfig())"
      ],
      "metadata": {
        "id": "MbyaqZbgztvq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  '''Transformer block: communication followed by computation'''\n",
        "  def __init__(self, config):\n",
        "      super().__init__()\n",
        "      self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "      self.attn = CausalSelfAttention(config)\n",
        "      self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "      self.mlp = MLP(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "XQ1AQvosxVzk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_obj = Block(GPTConfig())"
      ],
      "metadata": {
        "id": "QiCQL6PwyYI9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2 loading class"
      ],
      "metadata": {
        "id": "HAbUv4GzcwY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#class for loading gpt2 weights without having to use the huggingface library\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, targets = None):\n",
        "      # idx is of shape (B, T)\n",
        "      B, T = idx.size()\n",
        "      assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is {self.config.block_size}\"\n",
        "\n",
        "      # Forward the token and position embeddings\n",
        "      pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # shape (T)\n",
        "      pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (T, n_embd)\n",
        "      tok_emb = self.transformer.wte(idx)  # token embeddings of shape (B, T, n_embd)\n",
        "      x = tok_emb + pos_emb\n",
        "\n",
        "      # Forward the blocks of the transformer\n",
        "      for block in self.transformer.h:\n",
        "          x = block(x)\n",
        "\n",
        "      # Forward the final layernorm and the classifier\n",
        "      x = self.transformer.ln_f(x)\n",
        "      logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "      loss = None\n",
        "      if targets is not None:\n",
        "          loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "\n",
        "      return logits, loss\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"\n",
        "        Loads pretrained GPT-2 model weights from Huggingface\n",
        "        \"\"\"\n",
        "        assert model_type in ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
        "\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = dict(\n",
        "            gpt2=dict(n_layer=12, n_head=12, n_embd=768),   # 124M params\n",
        "            gpt2_medium=dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            gpt2_large=dict(n_layer=36, n_head=20, n_embd=1280),  # 774M params\n",
        "            gpt2_xl=dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        )[model_type]\n",
        "\n",
        "        config_args['vocab_size'] = 50257  # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024  # always 1024 for GPT model checkpoints\n",
        "\n",
        "        # Create a from-scratch initialized miniGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "\n",
        "        # Load state_dict from pretrained model\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]  # discard this mask / buffer\n",
        "\n",
        "        # Initialize a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # Copy while ensuring all the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]  # ignore these\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]  # same, just the mask\n",
        "\n",
        "        # Transposed keys for special handling\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "\n",
        "        assert len(sd_keys_hf) == len(sd_keys), \"Mismatched keys: {} vs {}\".format(len(sd_keys_hf), len(sd_keys))\n",
        "\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # Special treatment for the Conv1D weights that need to be transposed\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # Vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        model.load_state_dict(sd)\n",
        "        return model\n",
        "\n",
        "# Block and GPTConfig would need to be defined elsewhere in the code\n"
      ],
      "metadata": {
        "id": "4QjTuhJ2oJ00"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = GPT.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "jjFB0uqw7ZrR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #tiny shakespeare dataset\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "# with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "#     text = f.read()\n",
        "# data = text[:1000]\n",
        "# print(data[:100])"
      ],
      "metadata": {
        "id": "ROpRdqKrdbRP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wc input.txt"
      ],
      "metadata": {
        "id": "us9LxnmrkJqI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Train.py"
      ],
      "metadata": {
        "id": "Wr3jsoWTcmyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tb\n",
        "#Train.py code\n",
        "#attempt to autodetect the device\n",
        "import torch\n",
        "import tiktoken\n",
        "from torch.optim import AdamW\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "  device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "#device =\"cpu\" #OVERRIDE\n",
        "\n",
        "#tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "data = text[:1000]\n",
        "print(data[:100])\n",
        "\n",
        "#get a data batch\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "text = text[:1000]\n",
        "tokens = enc.encode(text)\n",
        "B, T = 4, 32\n",
        "buf = torch.tensor(tokens[:B*T+1])\n",
        "buf = buf.to(device)\n",
        "x = buf[:-1].view(B, T)\n",
        "y=buf[1:].view(B, T)\n",
        "\n",
        "#get logits\n",
        "config = GPTConfig()\n",
        "model = GPT(config)\n",
        "model.to(device)\n",
        "#x = x.to(device)\n",
        "#y = y.to(device)\n",
        "#logits, loss = model(x, y)\n",
        "\n",
        "#optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "for i in range(50):\n",
        "  logits, loss = model(x, y)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"step{i}, loss: {loss.item()}\")\n",
        "#print(logits.shape)\n",
        "#print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yVIEuSCecISA",
        "outputId": "3d560a52-1ca7-4f7d-ff48-75cccde30bed"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e082ce498b9a>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step{i}, loss: {loss.item()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#print(logits.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m                             )\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    225\u001b[0m             )\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    228\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "--2024-09-08 22:29:09--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "\rinput.txt.2           0%[                    ]       0  --.-KB/s               \rinput.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2024-09-08 22:29:09 (140 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "step0, loss: 11.197467803955078\n",
            "step1, loss: 6.776273727416992\n",
            "step2, loss: 5.539167404174805\n",
            "step3, loss: 3.1198935508728027\n",
            "step4, loss: 1.8629240989685059\n",
            "step5, loss: 1.0946894884109497\n",
            "step6, loss: 0.6052660942077637\n",
            "step7, loss: 0.3841351270675659\n",
            "step8, loss: 0.2615719735622406\n",
            "step9, loss: 0.17141036689281464\n",
            "step10, loss: 0.11937501281499863\n",
            "step11, loss: 0.08235868811607361\n",
            "step12, loss: 0.06435917317867279\n",
            "step13, loss: 0.05067714303731918\n",
            "step14, loss: 0.04041997343301773\n",
            "step15, loss: 0.034455135464668274\n",
            "step16, loss: 0.027238398790359497\n",
            "step17, loss: 0.023758506402373314\n",
            "step18, loss: 0.019589658826589584\n",
            "step19, loss: 0.016548074781894684\n",
            "step20, loss: 0.0146102299913764\n",
            "step21, loss: 0.012427080422639847\n",
            "step22, loss: 0.010811454616487026\n",
            "step23, loss: 0.009680528193712234\n",
            "step24, loss: 0.00877440720796585\n",
            "step25, loss: 0.008000862784683704\n",
            "step26, loss: 0.007336932234466076\n",
            "step27, loss: 0.0067693619057536125\n",
            "step28, loss: 0.006280283443629742\n",
            "step29, loss: 0.005851442459970713\n",
            "step30, loss: 0.005469241179525852\n",
            "step31, loss: 0.005125193390995264\n",
            "step32, loss: 0.004814406391233206\n",
            "step33, loss: 0.004534054081887007\n",
            "step34, loss: 0.004282076843082905\n",
            "step35, loss: 0.00405645277351141\n",
            "step36, loss: 0.0038549054879695177\n",
            "step37, loss: 0.0036750349681824446\n",
            "step38, loss: 0.003514367388561368\n",
            "step39, loss: 0.003370347199961543\n",
            "step40, loss: 0.0032407105900347233\n",
            "step41, loss: 0.0031234093476086855\n",
            "step42, loss: 0.003016603412106633\n",
            "step43, loss: 0.002918786369264126\n",
            "step44, loss: 0.002828809432685375\n",
            "step45, loss: 0.00274566188454628\n",
            "step46, loss: 0.0026685930788517\n",
            "step47, loss: 0.0025970202405005693\n",
            "step48, loss: 0.002530443947762251\n",
            "step49, loss: 0.0024684665258973837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "class DataloaderLite:\n",
        "  def __init__(self, B, T):\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "\n",
        "    #at init load tokens from disk and store them in memory\n",
        "    with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "      text = f.read()\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    tokens = enc.encode(text)\n",
        "    self.tokens = torch.tensors(tokens)\n",
        "    print(f\"loaded {len(tokens)} tokens\")\n",
        "    print(f\"1 epoch of data is {len(tokens)//(B*T)} batches\")\n",
        "\n",
        "    #state\n",
        "    self.current_position = 0\n",
        "\n",
        "  def next_batch(self):\n",
        "    B, T = self.B, self.T\n",
        "    buf = self.tokens[self.current_position:self.current_position+B*T+1]\n",
        "    x = buf[:-1].view(B, T) #inputs\n",
        "    y=buf[1:].view(B, T) #targets\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Yl5rKK5UmCpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "\n",
        "#model= GPT.from_pretrained('gpt2')\n",
        "model= GPT(GPTConfig())\n",
        "model.eval()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GjudeUt892x",
        "outputId": "58c3da32-5f13-4166-c68f-b06789d6d4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='tanh')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(data)\n",
        "print(tokens[:24])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwYH6QWInKgi",
        "outputId": "619b2eca-9bc0-47c8-a454-421cb537de35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating labels from tokens for GPT2\n",
        "import torch\n",
        "buf = torch.tensor(tokens[:24+1])\n",
        "x = buf[:-1].view(4,6)\n",
        "y=buf[1:].view(4,6)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuWd1MXsneq6",
        "outputId": "af7b2224-3e5c-4b24-e919-ff5592b0958c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
            "        [ 5120,   597,  2252,    11,  3285,   502],\n",
            "        [ 2740,    13,   198,   198,  3237,    25],\n",
            "        [  198,  5248,   461,    11,  2740,    13]])\n",
            "tensor([[22307,    25,   198,  8421,   356,  5120],\n",
            "        [  597,  2252,    11,  3285,   502,  2740],\n",
            "        [   13,   198,   198,  3237,    25,   198],\n",
            "        [ 5248,   461,    11,  2740,    13,   198]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prefix tokens\n",
        "\n",
        "#gpt roughly has a 3:1 compression ratio for characters to tokens\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(\"Hello, I'm a language model\", allowed_special={\"<|endoftext|>\"})\n",
        "tokens = torch.tensor(tokens, dtype=torch.long, device='cuda')\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "x = tokens.to(device)"
      ],
      "metadata": {
        "id": "M_RFceAJJVzD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "490b58ff-76f6-4498-bbc2-a8c47f36ee41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-17847a574ce4>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiktoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello, I'm a language model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_special\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"<|endoftext|>\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XwFJl4lnKh2V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}